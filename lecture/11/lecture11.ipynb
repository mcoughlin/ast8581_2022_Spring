{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'width': 1920, 'height': 1080, 'scroll': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook.services.config import ConfigManager\n",
    "cm = ConfigManager()\n",
    "cm.update('livereveal', {\n",
    "        'width': 1920,\n",
    "        'height': 1080,\n",
    "        'scroll': True,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 06 (Wednesday), AST 8581 / PHYS 8581 / CSCI 8581: Big Data in Astrophysics\n",
    "\n",
    "### Michael Coughlin <cough052@umn.edu>\n",
    "\n",
    "\n",
    "with contributions totally ripped off from Gautham Narayan (UIUC, Michael Steinbach (UMN), Nico Adams (UMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where do we stand?\n",
    "\n",
    "Foundations of Data and Probability -> Statistical frameworks (Frequentist vs Bayesian) -> Estimating underlying distributions -> Analysis of Time series (periodicity) -> Analysis of Time series (variability, continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Last Class: How do we know an object is variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# If $A=0$ (no variability)\n",
    "* ### $\\chi^2_{\\rm dof}=\\frac{1}{N} \\sum_j \\left(\\frac{y_j}{\\sigma}\\right)^2 \\sim V/\\sigma^2$\n",
    "* ### $\\chi^2_{\\rm dof}$ has  expectation value of 1 and std dev  of $\\sqrt\\frac{2}{N}$\n",
    "\n",
    "# If $|A|>0$ (variability)\n",
    "* ### $\\chi^2_{\\rm dof}$ will be larger\n",
    "than 1. \n",
    "* ### probability that $\\chi^2_{\\rm dof}>1 + 3 \\sqrt{2/N}$  is about 1 in 1000 (i.e., $>3\\sigma$ above 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Today: Stochastic Processes\n",
    "\n",
    "If a system is always variable, but the variability is not (infinitely) predictable, then we have a [**stochastic**](https://en.wikipedia.org/wiki/Stochastic) process.  \n",
    "\n",
    "Stochastic does not mean you cannot characterize this process, and make statistical statements about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"figures/quasar_schema.png\" style=\"float: left; width: 40%; margin-right: 1%;\"> <img src=\"figures/ngc4261-large.jpg\" style=\"float: left; width: 50%; margin-right: 1%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Take a (stochastically varying) quasar which has both *line* and *continuum* emission and where the line emission is stimulated by the continuum.  Since there is a physical separation between the regions that produce each type of emission, we get a delay between the light curves as can be seen here:\n",
    "\n",
    "![Peterson 2001, RM](https://ned.ipac.caltech.edu/level5/Sept01/Peterson2/Figures/figure24.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The correlation function\n",
    "\n",
    "Instead of the auto-correlation function, we can look at the more general correlation function - this gives us information about the time delay between 2 processes.  \n",
    "\n",
    "If one time series is derived from another simply by shifting the time axis by $t_{\\rm lag}$, then their correlation function will have a peak at $\\Delta t = t_{\\rm lag}$.\n",
    "\n",
    "The correlation function between $f(t)$, and $g(t)$ is defined as\n",
    "# $${\\rm CF}(\\Delta t) = \\frac{\\lim_{T\\rightarrow \\infty}\\frac{1}{T}\\int_T f(t)g(t+\\Delta t)dt }{\\sigma_f \\sigma_g}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Computing the correlation function is basically the mathematical processes of sliding the two curves over each other and computing the degree of similarity for each step in time.  \n",
    "\n",
    "The peak of the correlation function reveals the time delay between the processes.  Below we have the correlation function of the line and continuum emission from a quasar, which reveals a $\\sim$ 15 day delay between the two.\n",
    "\n",
    "![Peterson 2001, RM](https://ned.ipac.caltech.edu/level5/Sept01/Peterson2/Figures/figure25.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What can the ACF tell us?\n",
    "\n",
    "If the values of $y$ are uncorrelated, then ACF$(\\Delta t)=0$ (except for ACF$(0)=1$).\n",
    "\n",
    "For processes that \"retain memory\" of previous states only for some characteristic time $\\tau$, the ACF will vanish for $\\Delta t \\gg \\tau$.\n",
    "\n",
    "Turning that around, the predictability of future behavior of future behavior of such a process is limited to times up to $\\sim \\tau$; you have to \"let the process run\" to know how it will behave at times longer than that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Structure Function\n",
    "\n",
    "The *structure function* is another quantity that is frequently used in astronomy and is related to the ACF:\n",
    "\n",
    "# $${\\rm SF}(\\Delta t) = {\\sigma}_\\infty[1 - {\\rm ACF}(\\Delta t)]^{1/2}$$\n",
    "\n",
    "where ${\\sigma}_\\infty$ is the standard deviation of the time series as evaluated on timescales much larger than any charateristic timescale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The structure function is interesting because it's equal to the standard deviation of the distribution of the differences of $y(t_2) - y(t_1)$ evaluated at many different $t_1$ and $t_2$ (i.e., with a time lag of $\\Delta t = t_2 - t_1$), and divided by $\\sqrt 2$.\n",
    "\n",
    "This is of practical use: if I have a series of observations $y_i$ (taken at random times $t_i$) it's relatively straighforward to compute the structure function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Damped Random Walk\n",
    "\n",
    "A DRW is described by a stochastic differential equation which includes a damping term that pushes $y(t)$ back towards the mean, hence the name **damped random walk**.   \n",
    "\n",
    "The ACF for a DRW is given by\n",
    "\n",
    "# $$ ACF(t) = \\exp(-t/\\tau)$$\n",
    "where $\\tau$ is the characteristic timescale (i.e., the damping timescale).\n",
    "\n",
    "The DRW structure function can be written as\n",
    "# $$ SF(t) = \\sigma_{\\infty}[1-\\exp(-t/\\tau)]^{1/2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Notice what's happening\n",
    "\n",
    "We are **not** writing down a model for the observations directly anymore (they are a stochastic process so what'd be be the point)\n",
    "\n",
    "**We are writing down a model for how the observations are correlated with each other**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  Structure Function for Dampled Random Walk\n",
    "\n",
    "<img src=\"figures/MacLeod2010.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The SF example above was an example of a DRW: the light curve is strongly correlated a short timescales, but uncorrelated at long timescales. \n",
    "\n",
    "This is observed in optical variability of quasar continuum light; in fact, it works so well that one can use this model to distinguish quasars from stars, based solely on the variability they exhibit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you can make the ACF or SF, then you can jolly well take it's Fourier Transform to get the:\n",
    "\n",
    "### Power Spectral Density\n",
    "\n",
    "The Fourier Transform of an ACF is the [Power Spectral Density (PSD)](https://en.wikipedia.org/wiki/Spectral_density).  So, the PSD is an analysis in frequency space and the ACF is in time space.\n",
    "\n",
    "(the Wiener-Khinchin theorem describes the fact that the ACF and PSD are a Fourier pair) \n",
    "\n",
    "For example, for a sinusoidal function in time space, the ACF will have the same period, $T$. Conversly, the PSD in frequency space will be a $\\delta$ function centered on $\\omega = 1/2\\pi T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For our nice quasar without an analytic model but with an analytic form for the ACF, the PSD is then:\n",
    "\n",
    "# $$ PSD(f) = \\frac{\\tau^2 \\sigma_{\\infty}^2}{1+(2\\pi f \\tau)^2}$$\n",
    "\n",
    "which means that a DRW is a $1/f^2$ process at high frequency. The **damped** part comes from the flat PSD at low frequency.\n",
    "\n",
    "More generically, if \n",
    "\n",
    "## $${\\rm SF} \\propto t^{\\alpha}$$\n",
    "\n",
    "then \n",
    "\n",
    "## $${\\rm PSD} \\propto \\frac{1}{f^{1+2\\alpha}}$$\n",
    "\n",
    "So an analysis of a stochastic system can be done with either the ACF, SF, or PSD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Different stochastic processes can be categorized based on their ACF/PSD\n",
    "\n",
    "* A stochastic process with $1/f^2$ spectrum is known as random walk (if discrete) or Brownian motion (or, more accurately, Wiener process) if continuous. These physically occur when the value being observed is subjected to a series of independent changes of similar size. It's also sometimes called as \"red noise\". Quasar variability exhibits $1/f^2$ properties at high frequencies (that is, short time scales, below a year or so). \n",
    "\n",
    "* A stochastic process with $1/f$ spectrum are sometimes called \"long-term memory processes\" (also sometimes know as \"pink noise\"). They have equal energy at all octaves (or over any other logarithmic frequency interval). This type of process has infinite variance and an undefined mean (similar to a Lorentzian distribution). \n",
    "\n",
    "* A process with a constant PSD is frequently referred to as \"white noise\" -- it has equal intensity at all frequencies. This is a process with no memory -- each measurement is independent of all others. i.e. white noise is I.I.D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "If you get a time-series:\n",
    "\n",
    "# $$ (t_1, m_1), (t_2, m_2), ... , (t_N, m_n) $$\n",
    "\n",
    "- and you get luck enough to have a parametric model then you know what to do from first half of semester\n",
    "- you can say something about the **correlation structure** (*even if no explicit model for observations*)\n",
    "    - periodic processes\n",
    "        - periodogram to find strong periods\n",
    "        - can express as Fourier sum to construct a model and make predictions at future times\n",
    "    - stochastic processes\n",
    "        - ACF/SF\n",
    "        - PSD\n",
    "            - These are good for **characterizing** stochastic processes\n",
    "                - can use *features* for tasks like *classification* (soon! week 11!) \n",
    "                \n",
    "## But we're scientists and like to predict things (i.e. forecasting), not just characterize them - how do we do that for a generic time-series/process without an explicit model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Autoregressive Processes\n",
    "\n",
    "Processes (like time-series) that \"retain memory\" of previous states, can be described by [autoregressive models](https://en.wikipedia.org/wiki/Autoregressive_model).\n",
    "\n",
    "### You have already seen one of these:\n",
    "\n",
    "Our old friend the random walk - every new value is given by the preceeding value plus some noise:\n",
    "\n",
    "# $$y_i = y_{i-1} + \\epsilon_i$$\n",
    "\n",
    "If the coefficient of $y_{i-1}$ is $>1$ then it is known as a geometric random walk, which is typical of the stock market. These are **Markov Chains** \n",
    "\n",
    "(recall that not all Markov chains are stationary - they have to be positive recurrent and irreducible - i.e. you have to be able to get from every state to every other state in some finite time - it'd be dull if the stock market was stationary)\n",
    "\n",
    "So, if you interview for a quant position on Wall Street, you tell them that you are an expert in using autoregressive geometric random walks to model stochastic processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the random walk case above, each new value depends only on the immediately preceeding value.  But we can generalized this to include $p$ values:\n",
    "\n",
    "# $$y_i = \\sum_{j=1}^pa_jy_{i-j} + \\epsilon_i$$\n",
    "\n",
    "We refer to this as an [**autoregressive (AR)**](https://en.wikipedia.org/wiki/Autoregressive_model) process of order $p$: $AR(p)$.  \n",
    "\n",
    "For a random walk, we have $p=1$, and the weights are just $a_1=1$.\n",
    "\n",
    "If the data are drawn from a \"stationary\" process (one where it doesn't matter what region of the light curve you sample [so long as it is representative]), the $a_j$ satisfy certain conditions.\n",
    "\n",
    "One thing that we might do then is ask whether a system is more consistent with $a_1=0$ or $a_1=1$ (noise vs. a random walk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## An aside:\n",
    "\n",
    "You might wonder why the stock market is well described by a geometric random walk, and doesn't need an AR(10) or something. Recall that geometric random walks are also good at describing the motions of inebriated humans. Most economic theory is based on the assumption of people being \"Rational actors\", leading to \"Efficient Markets.\" \n",
    "\n",
    "Larry Summers would famously dispute this notion with the first line of a unpublished paper:\n",
    "\"There are idiots, look around.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are some example light curves for specific $AR(p)$ processes.  \n",
    "\n",
    "![AR Examples](https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/ArTimeSeries.svg/1000px-ArTimeSeries.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* $AR(0)$: the light curve is simply responding to noise fluctuations.  \n",
    "\n",
    "\n",
    "* $AR(1)$: the noise fluctuation responses are persisting for slightly longer as the next time step depends positively on the time before.  \n",
    "\n",
    "\n",
    "* $AR(1)$ w/ $a_1 = 0.9$: nearly the full effect of the noise spike from the previous time step is applied again, giving particularly long and high chains of peaks and valleys.  \n",
    "\n",
    "\n",
    "* $AR(2)$:  we have long, but low chains of peaks and valleys as a spike persists for an extra time step.  \n",
    "\n",
    "\n",
    "* $AR(2)$ w/ $a_1 = 0.9$ and $a_2 = -0.8$: the response of a spike in the second time step has the opposite sign as for the first time step - both have large coefficients - peaks and valleys are both quite high and quite narrowly separated.\n",
    "\n",
    "i.e. More general than simple periodic processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moving Average Processes\n",
    "\n",
    "A [**moving average (MA)**](https://en.wikipedia.org/wiki/Moving-average_model) process is similar to an AR process, but instead the value at each time step depends not on the *value* of previous time step, but rather the *perturbations* from previous time steps.  It is defined as\n",
    "\n",
    "# $$y_i = \\epsilon_i + \\sum_{j=1}^qb_j\\epsilon_{i-j}$$\n",
    "\n",
    "So, for example, an MA(q=1) process would look like\n",
    "\n",
    "# $$y_i = \\epsilon_{i} + b_1\\epsilon_{i-1},$$\n",
    "\n",
    "whereas an AR(p=2) process would look like\n",
    "\n",
    "# $$y_i = a_1y_{i-1} +  a_2y_{i-2} + \\epsilon_i$$\n",
    "\n",
    "\n",
    "### So, in an $MA$ process a shock affects only the current value and $q$ values into the future.  In an $AR$ process a shock affects *all* future values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## You can combine AR and MA processes. These are creatively called ARMA processes\n",
    "\n",
    "E.g. ARMA(2,1) model, which combines AR(2) and MA(1):\n",
    "\n",
    "\n",
    "# $$y_i = a_1y_{i-1} +  a_2y_{i-2} + \\epsilon_i + b_1 \\epsilon_{i-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-class Exercise: AR vs MA vs ARMA processes with a single shock\n",
    "\n",
    "Generate data from \n",
    "* an $AR(2)$ w/ $a_1 = 0.5, a_2=0.2$\n",
    "* an $MA(2)$ w/ $b_1 = 0.5, b_2=0.5$\n",
    "* an $ARMA(2, 1)$ w/ $a_1 = 0.5, a_2=0.25, b_1 =0.5$\n",
    "\n",
    "Add a \"shock\" (high epsilon value at t=3 - see how the different procsses respond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "N=20\n",
    "\n",
    "\n",
    "epsilon = np.random.randn(N+2) # complete me\n",
    "epsilon[3] = 15 # complete me\n",
    "\n",
    "\n",
    "yAR=np.zeros(N+2)\n",
    "yMA=np.zeros(N+2)\n",
    "yARMA=np.zeros(N+2)\n",
    "\n",
    "for i in np.arange(N)+2:\n",
    "    \n",
    "    \n",
    "    \n",
    "    yAR[i] = # complete me \n",
    "    yMA[i] = # complete me\n",
    "    yARMA[i] = # complete me\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "t = np.arange(len(yAR))\n",
    "plt.plot(t,yAR,label=\"AR(2), a_1=0.5, a_2=0.2\")\n",
    "plt.plot(t,yMA,label=\"MA(2), b_1=0.5, b_2=0.5\")\n",
    "plt.plot(t,yARMA,label=\"ARMA(2,1), a_1=0.5, a_2=0.25, b_1=0.5\",zorder=0)\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc=\"upper right\",prop={'size':8})\n",
    "\n",
    "ax = plt.axes()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "These videos might be useful:\n",
    "\n",
    "[MA(1)](https://www.youtube.com/watch?v=lUhtcP2SUsg)\n",
    "\n",
    "[AR(1)](https://www.youtube.com/watch?v=AN0a58F6cxA)\n",
    "\n",
    "[ARMA(1,1)](https://www.youtube.com/watch?v=Pg0RnP1uLVc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CARMA Models\n",
    "\n",
    "$AR$ and $ARMA$ models assume evenly sampled time-series data.  However, we can extend this to unevenly sampled data.\n",
    "\n",
    "These are **continuous** ARMA or CARMA models.\n",
    "\n",
    "\n",
    "A $CAR(1)$ process is described by a stochastic differential equation which includes a damping term that pushes $y(t)$ back towards the mean, so it is also a **damped random walk (DRW)**.  \n",
    "\n",
    "For evenly sampled data a CAR(1) process is the same as an AR(1) process with $a_1=\\exp(-1/\\tau)$.  \n",
    "\n",
    "That is, the next value is the previous value times the damping factor (plus noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Connecting this back to the ACF/SF/PSD\n",
    "\n",
    "The ACF for a DRW is given by\n",
    "# $$ ACF(t) = \\exp(-t/\\tau),$$\n",
    "where $\\tau$ is the characteristic timescale (i.e., the damping timescale).\n",
    "\n",
    "The structure function can be written as\n",
    "# $$ SF(t) = SF_{\\infty}[1-\\exp(-t/\\tau)]^{1/2}.$$\n",
    "\n",
    "The PSD is then\n",
    "# $$ PSD(f) = \\frac{\\tau^2 SF_{\\infty}^2}{1+(2\\pi f \\tau)^2},$$\n",
    "\n",
    "\n",
    "which means that a DRW is a $1/f^2$ process at high frequency.  \n",
    "\n",
    "The *damped* part comes from the flat PSD at low frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# In-class Exercise: Stochastic Processes and the ACF\n",
    "\n",
    "AstroML has [time series](http://www.astroml.org/modules/classes.html#module-astroML.time_series) and [Fourier](http://www.astroml.org/modules/classes.html#module-astroML.fourier) tools for generating light curves drawn from a power law in frequency space.  \n",
    "\n",
    "Note that these tools define $\\beta = 1+2\\alpha$  ($\\beta=2$ for a random walk). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# RUN THIS\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from astroML.time_series import generate_power_law\n",
    "from astroML.fourier import PSD_continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N = 2014\n",
    "dt = 0.01\n",
    "\n",
    "\n",
    "betaRed = 2  # Complete\n",
    "betaPink = 1  # Complete\n",
    "betaWhite = 0  # Complete\n",
    "\n",
    "t = dt * np.arange(N)\n",
    "yRed = generate_power_law(N, dt, betaRed)\n",
    "yPink = # Complete\n",
    "yWhite = # Complete\n",
    "\n",
    "\n",
    "\n",
    "fRed, PSDred = # Complete\n",
    "fPink, PSDpink = # Complete\n",
    "fWhite, PSDwhite = # Complete\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.plot(t, yWhite, c='Grey')\n",
    "ax1.plot(t, yPink, c='Pink')\n",
    "ax1.plot(t, yRed, '-r')\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_title('Real Space')\n",
    "ax1.set_xlabel('time')\n",
    "ax1.set_ylabel('fake mag')\n",
    "\n",
    "ax2 = fig.add_subplot(122, xscale='log', yscale='log')\n",
    "ax2.plot(fWhite, PSDwhite, c='Grey')   \n",
    "ax2.plot(fPink, PSDpink, c='Pink')  \n",
    "ax2.plot(fRed, PSDred, '-r')  \n",
    "ax2.set_xlim(1E-1, 60)\n",
    "ax2.set_ylim(1E-11, 1E-3)\n",
    "ax2.set_xlabel('Frequency')\n",
    "ax2.set_xlabel('Power')\n",
    "ax2.set_title('PSD')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ACF for Unevenly Sampled Data\n",
    "\n",
    "astroML also has tools for computing the ACF of unevenly sampled data using two different (Scargle) and (Edelson & Krolik) methods: [http://www.astroml.org/modules/classes.html#module-astroML.time_series](http://www.astroml.org/modules/classes.html#module-astroML.time_series)\n",
    "\n",
    "One of the tools is for generating a **damped random walk (DRW)**.  Above we found that a random walk had a $1/f^2$ PSD.  A *damped* random walk is a process \"remembers\" its history only for a characteristic time, $\\tau$. The ACF vanishes for $\\Delta t \\gg \\tau$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: AstroMLDeprecationWarning: The lomb_scargle function is deprecated and may be removed in a future version.\n",
      "        Use astropy.stats.LombScargle instead. [astroML.time_series.ACF]\n",
      "WARNING: AstroMLDeprecationWarning: The lomb_scargle function is deprecated and may be removed in a future version.\n",
      "        Use astropy.stats.LombScargle instead. [astroML.time_series.ACF]\n"
     ]
    }
   ],
   "source": [
    "# Syntax for EK and Scargle ACF computation\n",
    "import numpy as np\n",
    "from astroML.time_series import generate_damped_RW\n",
    "from astroML.time_series import ACF_scargle, ACF_EK\n",
    "\n",
    "t = np.arange(0,1000)\n",
    "y = generate_damped_RW(t, tau=300)\n",
    "dy = 0.1\n",
    "y = np.random.normal(y,dy)\n",
    "\n",
    "ACF_scargle, bins_scargle = ACF_scargle(t,y,dy)\n",
    "ACF_EK, ACF_err_EK, bins_EK = ACF_EK(t,y,dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ivezic, Figure 10.30\n",
    "# Author: Jake VanderPlas\n",
    "# License: BSD\n",
    "#   The figure produced by this code is published in the textbook\n",
    "#   \"Statistics, Data Mining, and Machine Learning in Astronomy\" (2013)\n",
    "#   For more information, see http://astroML.github.com\n",
    "#   To report a bug or issue, use the following forum:\n",
    "#    https://groups.google.com/forum/#!forum/astroml-general\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from astroML.time_series import lomb_scargle, generate_damped_RW\n",
    "from astroML.time_series import ACF_scargle, ACF_EK\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Generate time-series data:\n",
    "#  we'll do 1000 days worth of magnitudes\n",
    "\n",
    "t = np.arange(0, 1E3)\n",
    "z = 2.0\n",
    "tau = 300\n",
    "tau_obs = tau / (1. + z)\n",
    "\n",
    "np.random.seed(6)\n",
    "y = generate_damped_RW(t, tau=tau, z=z, xmean=20)\n",
    "\n",
    "# randomly sample 100 of these\n",
    "ind = np.arange(len(t))\n",
    "np.random.shuffle(ind)\n",
    "ind = ind[:100]\n",
    "ind.sort()\n",
    "t = t[ind]\n",
    "y = y[ind]\n",
    "\n",
    "# add errors\n",
    "dy = 0.1\n",
    "y_obs = np.random.normal(y, dy)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute ACF via scargle method\n",
    "C_S, t_S = ACF_scargle(...\n",
    "\n",
    "ind = (t_S >= 0) & (t_S <= 500)\n",
    "t_S = t_S[ind]\n",
    "C_S = C_S[ind]\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# compute ACF via E-K method\n",
    "C_EK, C_EK_err, bins = ACF_EK(...\n",
    "t_EK = 0.5 * (bins[1:] + bins[:-1])\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the results\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "# plot the input data\n",
    "\n",
    "# plot the ACF"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
