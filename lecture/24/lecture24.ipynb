{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqu_lZAIAdfF"
   },
   "source": [
    "# Week 13 (Wednesday), AST 8581 / PHYS 8581 / CSCI 8581: Big Data in Astrophysics\n",
    "\n",
    "### Ville Cantory <canto063@umn.edu>, Michael Coughlin <cough052@umn.edu>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAKBcTLQAdfM"
   },
   "source": [
    "# Where do we stand?\n",
    "\n",
    "Foundations of Data and Probability -> Statistical frameworks (Frequentist vs Bayesian) -> Estimating underlying distributions -> Analysis of Time series (periodicity) -> Analysis of Time series (variability) -> Analysis of Time series (stochastic processes) -> Gaussian Processes -> Decision Trees / Regression -> Dimensionality Reduction -> Principle Component Analysis -> Clustering -> Density Estimation / Anomaly Detection -> Supervised Learning -> Deep Learning -> Introduction to Databases - SQL -> Introduction to Databases - NoSQL -> Introduction to Multiprocessing -> Introduction to GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMce8muBqXQP"
   },
   "source": [
    "# Tensorflow with GPU\n",
    "\n",
    "This notebook provides an introduction to computing on a [GPU](https://cloud.google.com/gpu) in Colab. In this notebook you will connect to a GPU, and then run some basic TensorFlow operations on both the CPU and a GPU, observing the speedup provided by using the GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM_8ELnJq_wd"
   },
   "source": [
    "## Enabling and testing the GPU\n",
    "\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "Next, we'll confirm that we can connect to the GPU with tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXnDmXR7RDr2",
    "outputId": "f5106db2-1184-4ce6-f029-8567bf836790"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rLiJPLY0tjvs"
   },
   "outputs": [],
   "source": [
    "@vectorize(['float64(float64, float64)'], target='cuda')\n",
    "def vecCuda_func(a, b):\n",
    "  for i in range(100000):\n",
    "    a = math.pow(a*b, 1./2)/math.exp(a*b/1000)\n",
    "  return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "naP5t1btub6C",
    "outputId": "de3de4ca-4eb3-429f-a236-933c7a63af4b"
   },
   "outputs": [],
   "source": [
    "%timeit res = vecCuda_func(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymlt0rKIuhf-"
   },
   "outputs": [],
   "source": [
    "# Woah!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1VRyAOtzrYD"
   },
   "source": [
    "# 6. Running your functions on GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "J3_Zv57yxsBV"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def cudaKernal_func(a, b, result): # cuda.jit does not return result yet\n",
    "  pos = cuda.grid(1)\n",
    "  if (pos < a.shape[1]) and (pos < b.shape[0]):\n",
    "    for i in range(100000):\n",
    "      result[pos] = math.exp(a[0][pos]*b[pos][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "S4pShvAI1YjR"
   },
   "outputs": [],
   "source": [
    "result = np.zeros((100,), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RxlLDdbT1gFf",
    "outputId": "bb352087-ad20-4a5b-9a18-ec0abe13d3e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 6.39 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 5: 55.1 ms per loop\n"
     ]
    }
   ],
   "source": [
    "threadsperblock = 32\n",
    "blockspergrid = (100 + 31) // 32 # blockspergrid = (array.size + (threadsperblock - 1)) // threadsperblock\n",
    "\n",
    "%timeit cudaKernal_func[threadsperblock, blockspergrid](a, b, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I0Q2FOAk1rXw",
    "outputId": "cdf3745c-c3cb-4589-ad26-2f9a26156667"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DjchmIF13_j"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Here, we have only used it for 1D arrays. You can use it for any Tensor. For eg:\n",
    "# For 2D array operations you would have used: x, y = cuda.grid(2)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "e1snDXNG3c9A"
   },
   "outputs": [],
   "source": [
    "@cuda.jit(device=True)\n",
    "def cudaDevice_func(a, b):\n",
    "  for i in range(100000):\n",
    "    a = math.exp(a*b)\n",
    "  return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dgv5aV6n3wgv"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def cudaKernal_func2(a, b, result): # cuda.jit does not return result yet\n",
    "  pos = cuda.grid(1)\n",
    "  if (pos < a.shape[1]) and (pos < b.shape[0]):\n",
    "    result[pos] = cudaDevice_func(a[0][pos], b[pos][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ro7kzwPS32vv",
    "outputId": "7696bc92-bd21-4ef4-a288-579a5b7a95fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 6.20 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 5: 58.5 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit cudaKernal_func2[threadsperblock, blockspergrid](a, b, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6p1OqLnJ-7H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lggcAtkx4kMl"
   },
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T37JiXS0eh2R"
   },
   "source": [
    "### Torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0NoRkDUApifl"
   },
   "outputs": [],
   "source": [
    "!pip -q install torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OI1TCDY0ej6a"
   },
   "source": [
    "### Dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HK2yYT9MzTw"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/49853303/how-to-install-pydot-graphviz-on-google-colab?rq=1\n",
    "!pip -q install graphviz \n",
    "!apt-get install graphviz -qq\n",
    "!pip -q install pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKhW3jTB4m6u"
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSfGRxu64oLZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, Process\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bEvBwSsoQfj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7435zkZpRyj"
   },
   "source": [
    "# 5. Pycuda (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SvcFFniYahLl"
   },
   "outputs": [],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "from pycuda.compiler import SourceModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAMWyy3xyT72"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "a = numpy.random.randint(1, 9, (10000, 10000))\n",
    "a = a.astype(numpy.float32) # Most nVidia devices support single precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuMJGXOTzDtn"
   },
   "outputs": [],
   "source": [
    "# The main drawback of Pycuda is that you will have to write C code\n",
    "# to perform any task and pass it to SourceModule...\n",
    "\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void doublify(float *a)\n",
    "  {\n",
    "    int idx = threadIdx.x + threadIdx.y*4;\n",
    "    a[idx] *= 2;\n",
    "  }\n",
    "  \"\"\")\n",
    "\n",
    "# As we saw in Numba post, you have to know which thread we are in to compute index(s) to operate on.\n",
    "# Here we get that from threadId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYVDamqZ10gu"
   },
   "outputs": [],
   "source": [
    "# Get reference to function and initialize it, passing array and block size (4x4).\n",
    "func = mod.get_function(\"doublify\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqXL4t-V2Ah4"
   },
   "outputs": [],
   "source": [
    "a_gpu = cuda.mem_alloc(a.nbytes) # Allocate memory on GPU\n",
    "cuda.memcpy_htod(a_gpu, a) # Send array to GPU. Now your array is within variable a_gpu\n",
    "a_gpu = cuda.to_device(a)\n",
    "func(a_gpu, block=(4, 4, 1))\n",
    "a_doubled = numpy.empty_like(a) # Copy array from gpu to cpu\n",
    "cuda.memcpy_dtoh(a_doubled, a_gpu)\n",
    "a_doubled[0][0:10], a[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AaNr4gON2Tuh"
   },
   "outputs": [],
   "source": [
    "# We can also simply use cuda.InOut which sends array to GPU\n",
    "# and then retrives back again:\n",
    "func(cuda.InOut(a), block=(4, 4, 1)) # block takes Block size which can be 3D. Block size gives us number of threads in Block. (No. of threads should be <= 512)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jG7mqYikv-0c"
   },
   "outputs": [],
   "source": [
    "# Whereas a grid is 2D and can have max of 1000's of blocks. As before its dimensions give number of blocks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORmOJq4MvKib"
   },
   "source": [
    "#### Examples:\n",
    "\n",
    "Image: https://raw.githubusercontent.com/andreajeka/CUDAThreadIndexing/master/images/1dgrid3dblock.png\n",
    "\n",
    "\n",
    "As shown in image, if we use 1D grid and 3D blocks, that is how you can think of it. Each of the thread sub-blocks contain one thread. Structure like this makes indexing of threads in grid easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "sZbfoqjveSbV",
    "outputId": "ff0ece66-1a00-4fd2-fc6c-68afc4dbfabd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([127.,   0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,\n",
       "        10.,  11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,\n",
       "        21.,  22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# 1D grid and 1D block\n",
    "#\n",
    "\n",
    "temp = numpy.zeros((4000,))\n",
    "temp = temp.astype(numpy.float32)\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void cuda_func(float *a)\n",
    "  {\n",
    "    int idx = blockDim.x*blockIdx.x + threadIdx.x;  // Go to block + Go to thread\n",
    "    if(idx < 4000){\n",
    "       a[idx] = (float) threadIdx.x; // One thread operating on one index. There are total of (4127)//128 = 32 blocks, each having 128 threads. Total 4096 threads.\n",
    "    }\n",
    "  }\n",
    "  \"\"\")\n",
    "func = mod.get_function(\"cuda_func\")\n",
    "func(cuda.InOut(temp), grid=((4000+127)//128, 1), block=(128, 1, 1))\n",
    "temp[-33:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "ZVvC9b6Njh2K",
    "outputId": "c1b7e6f7-2046-46a3-ad0a-dc4d47542a0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6., 7., 0., 1., 2., 3., 4., 5., 6., 7.], dtype=float32),\n",
       " array([2., 2., 3., 3., 3., 3., 3., 3., 3., 3.], dtype=float32))"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# 1D grid and 2D block\n",
    "#\n",
    "\n",
    "temp = numpy.zeros((4000,))\n",
    "temp2 = numpy.zeros((4000,))\n",
    "temp = temp.astype(numpy.float32)\n",
    "temp2 = temp2.astype(numpy.float32)\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void cuda_func(float *a, float *b)\n",
    "  {\n",
    "    int idx = blockDim.x*blockDim.y*blockIdx.x + blockDim.x* threadIdx.y + threadIdx.x;  // Go to block + Go to row to current thread + Go to thread\n",
    "    if(idx < 4000){\n",
    "       a[idx] = (float) threadIdx.x; // One thread operating on one index. There are total of (4063)//64 = 63 blocks, each having (8x8)=64 threads. Total 4032 threads.\n",
    "       b[idx] = (float) threadIdx.y;\n",
    "    }\n",
    "  }\n",
    "  \"\"\")\n",
    "func = mod.get_function(\"cuda_func\")\n",
    "func(cuda.InOut(temp), cuda.InOut(temp2), grid=((4000+63)//64, 1), block=(8, 8, 1))\n",
    "temp[-10:], temp2[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "fyUsLwBJoH4d",
    "outputId": "30827c24-a3ff-44f7-c895-3313df44ae06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 0., 1., 2., 3., 0., 1., 2., 3., 0., 1., 2., 3., 0., 1., 2., 3.],\n",
       "       dtype=float32),\n",
       " array([3., 0., 0., 0., 0., 1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.],\n",
       "       dtype=float32),\n",
       " array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# 1D grid and 3D block\n",
    "#\n",
    "\n",
    "temp = numpy.zeros((4000,))\n",
    "temp = temp.astype(numpy.float32)\n",
    "temp2 = numpy.zeros((4000,))\n",
    "temp2 = temp2.astype(numpy.float32)\n",
    "temp3 = numpy.zeros((4000,))\n",
    "temp3 = temp3.astype(numpy.float32)\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void cuda_func(float *a, float *b, float *c)\n",
    "  {\n",
    "    int idx = blockDim.x*blockDim.y*blockDim.z*blockIdx.x +  // Go to block \n",
    "              blockDim.x*blockDim.y*threadIdx.z +            // Go to z sclice containing thread\n",
    "              blockDim.x*threadIdx.y +                       // In that slice go to row containing thread\n",
    "              threadIdx.x;                                   // Go to thread\n",
    "    if(idx < 4000){\n",
    "       a[idx] = (float) threadIdx.x; // One thread operating on one index. There are total of (4063)//64 = 63 blocks, each having (4x4x4)=64 threads. Total 4032 threads.\n",
    "       b[idx] = (float) threadIdx.y;\n",
    "       c[idx] = (float) threadIdx.z;\n",
    "    }\n",
    "  }\n",
    "  \"\"\")\n",
    "func = mod.get_function(\"cuda_func\")\n",
    "func(cuda.InOut(temp), cuda.InOut(temp2), cuda.InOut(temp3), grid=((4000+63)//64, 1), block=(4, 4, 4))\n",
    "temp[-17:], temp2[-17:], temp3[-17:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "id": "BAqVLfRRh5nL",
    "outputId": "734bdc99-9986-47fc-dc0c-713bae2c8e93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3., 0., 1., 2., 3., 0., 1., 2., 3., 0., 1., 2., 3., 0., 1., 2., 3.],\n",
       "       dtype=float32),\n",
       " array([3., 0., 0., 0., 0., 1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.],\n",
       "       dtype=float32),\n",
       " array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       dtype=float32),\n",
       " array([6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6., 6.],\n",
       "       dtype=float32),\n",
       " array([7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7., 7.],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# 2D grid and 3D block\n",
    "#\n",
    "\n",
    "temp = numpy.zeros((4000,))\n",
    "temp = temp.astype(numpy.float32)\n",
    "temp2 = numpy.zeros((4000,))\n",
    "temp2 = temp2.astype(numpy.float32)\n",
    "temp3 = numpy.zeros((4000,))\n",
    "temp3 = temp3.astype(numpy.float32)\n",
    "temp4 = numpy.zeros((4000,))\n",
    "temp4 = temp4.astype(numpy.float32)\n",
    "temp5 = numpy.zeros((4000,))\n",
    "temp5 = temp5.astype(numpy.float32)\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void cuda_func(float *a, float *b, float *c, float *d, float *e)\n",
    "  {\n",
    "    int idx = gridDim.x*blockDim.x*blockDim.y*blockDim.z*blockIdx.y +      // Go to grid row containing block\n",
    "              blockDim.x*blockDim.y*blockDim.z*blockIdx.x +                // Go to block\n",
    "              blockDim.x*blockDim.y*threadIdx.z +                          // Go to slice of block containing thread\n",
    "              blockDim.x*threadIdx.y +                                     // In that slice go to row containing thread\n",
    "              threadIdx.x;                                                 // Go to thread\n",
    "    if(idx < 4000){\n",
    "       a[idx] = (float) threadIdx.x; // One thread operating on one index. There are total of (8*8) = 64 blocks, each having (4x4x4)=64 threads. Total 4096 threads.\n",
    "       b[idx] = (float) threadIdx.y;\n",
    "       c[idx] = (float) threadIdx.z;\n",
    "       d[idx] = (float) blockIdx.x;\n",
    "       e[idx] = (float) blockIdx.y;\n",
    "    }\n",
    "  }\n",
    "  \"\"\")\n",
    "func = mod.get_function(\"cuda_func\")\n",
    "func(cuda.InOut(temp), cuda.InOut(temp2), cuda.InOut(temp3), cuda.InOut(temp4), cuda.InOut(temp5), grid=(8, 8), block=(4, 4, 4))\n",
    "temp[-17:], temp2[-17:], temp3[-17:], temp4[-17:], temp5[-17:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "id": "SmlbQLcq3VDy",
    "outputId": "075eb450-fe7c-413d-db8f-6c16a483137f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3990., 3991., 3992., 3993., 3994., 3995., 3996., 3997., 3998.,\n",
       "       3999.], dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# 1D grid and 1D block and only 1 thread\n",
    "#\n",
    "\n",
    "temp = numpy.zeros((4000,))\n",
    "temp = temp.astype(numpy.float32)\n",
    "mod = SourceModule(\"\"\"\n",
    "  __global__ void cuda_func(float *a)\n",
    "  {\n",
    "    printf(\"%d\", threadIdx.x);\n",
    "    for(int i=0; i<4000; i++){\n",
    "      a[i] = i;\n",
    "    }\n",
    "  }\n",
    "  \"\"\")\n",
    "func = mod.get_function(\"cuda_func\")\n",
    "func(cuda.InOut(temp), grid=(1, 1), block=(1, 1, 1))\n",
    "temp[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9KBHlvd4Gk9"
   },
   "outputs": [],
   "source": [
    "# So on a single thread you can operate on an subarray too. You will have to configure task accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZQ1dxF6J-7T"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "lecture24.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
